# Парсер Wildberries

Представляю Вам небольшой парсер маркетплейса Wildberries.
Код обновлен: 27.02.2023

### Что он парсит?

Парсит определенную категорию, например мужские футболки, брюки и т.д.

- id товара
- название товара
- id бренда
- название бренда
- полная стоимость товара (зачеркнутая цена)
- актуальная цена товара
- количество отзывов

### Что можно спарсить еще?

Спарсить, мне кажется, можно почти все. Вопрос прикладываемых усилий. Если почти не прикладывать, то можно быстро спарсить:

- рейтинг товара
- доступные размеры
- доступные расцветки
- промо
- картинки товара

Почему не прикладывать усилий? Все эти данные находятся в одном json, поэтому единственное, что Вам требуется - это добавить по примеру остальных категорий нужные Вам. Там есть и другие индикаторы, но мне не пришлось с ними работать, поэтому я точно не могу сказать на какие именно данные они указывают.

### Что нужно знать еще?

У Wildberries есть некоторые особенности. Чтобы спарсить абсолютно все товары - придется постараться, потому что по одному URL маркетплейс выдает максимум 100 страниц или 10.000 товаров. Что я имею ввиду: как мы парсим пагинацию? Создаем цикл, который подставляет номер страницы в нужный фрагмент URL и парсим каждую страницу. Так вот, если товаров в нужной Вам категории больше 10.000, а таких категорий там большая часть (футболок, например, там ~ 400.000), то когда Вы попробуете попасть через URL на 101 страницу, то никаких товаров Вы не получите, будет что-то вроде 404 ошибки. Что интересно, если перейти на 101 страницу через пагинацию сайта, то она откроется. Если Вы перейдете на 102, а потом вернетесь назад, то снова не получите никаких товаров.
Что делать? Использовать фильтры. Вы можете выбрать удобный себе вариант, но мой совет - использовать ценовой фильтр. Чтобы в рамках диапазона этой цены не было товаров количеством более 10.000, так Вы идете по фильтру до последнего товара. Я сам проверял, на сегодняшний день - это рабочий способ, в URL повышаете циклом цену, вопрос оптимального диапазона.
Ниже я покажу как достать ссылку на данные в общем порядке и с фильтрами.
Да, здесь очень много AJAX. Если Вы привыкли доставать код из HTML, то здесь готовьтесь жить во вкладке Network.

### Реализация

Все данные записываются в *.csv файл для дальнейшей работы с ними.

##### Proxy

Парсятся с любого ресурса free proxy (не забывайте, что если Вы будете использовать отличный от кода прокси лист, то Вам придется делать парсер прокси под этот конкретный ресурс!), формируются сразу в коде и подставляются. Есть ресурсы, где прокси находятся в json, что возможно даже упростит код. У бесплатных прокси есть одна проблема - их качество, что непосредственно влияет на работу парсера, поэтому если Вам действительно нужен прокси, то советую уделить этому моменту больше времени. Скажу так, по моим наблюдениям прокси в данном парсере - не нужен, но Вы можете изучить реализацию, чтобы применить в других проектах.

Если будут возникать ошибки с прокси, то советую изучить Вам эти 2 вопроса:
https://stackoverflow.com/questions/26109264/pip-proxy-authentication-and-not-supported-proxy-scheme
https://stackoverflow.com/questions/66642705/why-requests-raise-this-exception-check-hostname-requires-server-hostname

### Multiprocessing

А вот это уже полезная вещь. Класс Pool() принимает на вход целое число - количество процессов Python, которые будут запущены. Но стоит учитывать, что не всегда Вы масштабируете скорость с количеством процессоров. Иногда сервер просто не успевает отвечать на такое количество запросов (если их много), поэтому если в одном процессе Вы спарсили что-то за 10 минут, а потом использовали Pool(20) и время уменьшилось не до 30 секунд, то возможно проблема как раз в самом сервере, куда Вы делаете запросы.
Не всегда это дает ожидаемые результаты, но сам модуль советую к изучению, хоть и не нахожу его простым.

### Общие моменты

В коде Вас ждет конкретная ссылка на конкретную категорию. Ее можно сделать генерируемой, если Вам надо, например, спарсить абсолютно весь Wildberries. Как я писал выше, через нее же можно сделать генерацию страниц по ценовому диапазону.
Чтобы получить данный Request URL Вам нужно:

- F12
- открыть нужною категорию
- перейти через пагинацию на другю страницу
- найти вкладку 'catalog?appType=1&cat=8144&couponsGeo?'
- перейти по URL в Request URL

Там вас будет ждать json, содержащий все товары на данной странице. Получать ссылку с фильтрами - точно также. Выставляете фильтры, смотрите количество товаров по фильтру и снова повторяете алгоритм. Если Вы выбрали фильтр, где меньше 100 товаров (т.е. не будет пагинации на вторую страницу), то выберите другой фильтр, откройте ссылку с json и уже в URL корректируйте диапазон.

Думаю на этом все, но если у Вас возникли вопросы или требуется помощь - обращайтесь в телеграм @fouraftermidnight или на почту novichkovroman.py@gmail.com

